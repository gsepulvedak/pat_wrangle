{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This work comprises the execution of different text processing and analysis tasks applied to patent documents in XML format. The required task is to extract the __grant_id, patent_kind, patent_title, number_of_claims, citations_examiner_count, citations_applicant_count, inventors, claims_text__ and __abstract__ features from a given text file and store them into a tidy dataset in __CSV__ and __JSON__ format. More details for each task will be given in the following sections.\n",
    "\n",
    "External libraries allowed: Regular Expressions & Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.072080Z",
     "start_time": "2020-07-14T09:56:57.069173Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, the file Group135.txt was examined in order to get familiar with it's structure and main features. This was done not only using python but also making use of text editors such as `vim` and the text viewer feature of `PyCharm`.\n",
    "\n",
    "The first lines of the input file are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.128006Z",
     "start_time": "2020-07-14T09:56:57.076585Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<us-patent-grant lang=\"EN\" dtd-version=\"v4.5 2014-04-03\" file=\"US10357528-20190723.XML\" status=\"PRODUCTION\" id=\"us-patent-grant\" country=\"US\" date-produced=\"20190709\" date-publ=\"20190723\">\n",
      "<us-bibliographic-data-grant>\n",
      "<publication-reference>\n",
      "<document-id>\n",
      "<country>US</country>\n",
      "<doc-number>10357528</doc-number>\n",
      "<kind>B2</kind>\n",
      "<date>20190723</date>\n",
      "</document-id>\n",
      "</publication-reference>\n",
      "<application-reference appl-type=\"utility\">\n",
      "<document-id>\n",
      "<country>US</country>\n",
      "<doc-number>15742391</doc-number>\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the first lines of the input file\n",
    "file_handler = open('Group135.txt', mode = 'r')\n",
    "input_file = file_handler.readlines()\n",
    "file_handler.close()\n",
    "for line in input_file[:15]:\n",
    "    print(line, end ='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the first 15 lines, the document seems to be properly formatted. This allows to find and obtain the information of interest using regular expressions.\n",
    "Some of the required information can be already seen in the first 10 lines, such as the grant_id as a combination of the data under `<country>` and `<doc-number>` tags, and what it seems to be a code for patent_kind under the `<kind>` tag.\n",
    "\n",
    "From the previous result we see the string `<?xml version=\"1.0\" encoding=\"UTF-8\"?>` at the beggining of a given section of a particular patent grant data. Further inspection of the file using text editors confirmed that each particular section of patent grant data in fact starts with this. Hence, this string is a good candidate for splitting the input file to separate in different items the patents data.\n",
    "\n",
    "In order to separate each patent data in items in a list, the following was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.214963Z",
     "start_time": "2020-07-14T09:56:57.129743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Opening the input data file\n",
    "# File is read as a large string, then the '\\n' character is eliminated to facilitate the use of regular expressions.\n",
    "# Finally the string file is split, separating each patent grant in a single element in a list.\n",
    "\n",
    "file_handler = open('Group135.txt', mode = 'r')\n",
    "input_file = file_handler.read().replace('\\n', '')\n",
    "file_handler.close()\n",
    "data = input_file.split('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parsing Group135.txt File\n",
    "\n",
    "Making use of text editors and sample output files provided, the location and xml tags for every feature of interest were identified. Then, a general approach was applied to extract the desired data:\n",
    "1. Extracting a smaller section of the whole patent text where the data of interest is nested.\n",
    "2. Extracting the data of interest.\n",
    "3. Performing some data manipulation to get to the desired format, according to the sample files.\n",
    "\n",
    "Extracting a smaller section of the text prior to get the actual data (step 1) was done in order to avoid the regular expressions extracting unwanted data under the same tag structure from wrong sections of the patent grant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### grant_id feature\n",
    "The grant_id data was found nested inside `<publication-reference>` tag, specifically as a combination of the data under `<country>` and `<doc-number>` tags. Given so, the general approach was applied and a function `getGrantId` was defined for extracting the wanted data. The function returns a string where country and doc-number are concatenated.\n",
    "Regex captures 2 groups of the grant_id: a 'letter part' which is in between `<country>` and `</co` tags and a 'numeric part' in between the `</co.+?number>(.+?)</doc` pattern. Then both groups are joined into one ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.219284Z",
     "start_time": "2020-07-14T09:56:57.216329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate grant_id data\n",
    "def getGrantId(data):\n",
    "    # Extracting desired sub section of text\n",
    "    section_pattern = r'<publication-reference.+?>.+?</publication-reference>' \n",
    "    text_section = re.findall(section_pattern, data)\n",
    "    \n",
    "    # Extracting wanted data\n",
    "    grant_id_pattern = r'<country>(.+?)</co.+?number>(.+?)</doc'\n",
    "    grant_id = re.findall(grant_id_pattern, ''.join(text_section))\n",
    "    grant_id = ''.join(grant_id[0]) # Convert output list of tuples into string\n",
    "    return str(grant_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### patent_title feature\n",
    "This information was located under the tag `<invention-title>`. A function `getPatentTitle` was created, which returns different outputs according to the `fileType` argument. This argument was used to allow the function return appropiate output for writing into a csv file or into a json file, adding `\"` around the returned string when necessary. Also, the function considers eliminating html tags found on patent_title data, making use of the regular expression `<.+?>` (the method used for finding html tags and special xml characters is detailed in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.226426Z",
     "start_time": "2020-07-14T09:56:57.221018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract patent_title feature\n",
    "def getPatentTitle(data, fileType = 'csv'):\n",
    "    patent_title_pattern = r'<invention-title.+?>(.+?)</invention-title>'\n",
    "    patent_title = re.findall(patent_title_pattern, data)\n",
    "    patent_title = ''.join(patent_title[0])\n",
    "    \n",
    "    # Eliminating tags\n",
    "    patent_title = re.sub(r'<.+?>', '', patent_title)\n",
    "    \n",
    "    # Formatting for different output files\n",
    "    if ',' in patent_title and fileType != 'json':\n",
    "        patent_title = '\"' + patent_title + '\"'\n",
    "    elif '/' in patent_title and fileType == 'json':\n",
    "        patent_title = re.sub(r'/', '\\/', patent_title)\n",
    "    return patent_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### kind feature\n",
    "The kind feature data was nested inside `<publication-reference>` section, under the tag `<kind>`. The data was actually a code for a longer description. A dictionary was used to translate the code value to the needed string. The dictionary was elaborated based on the sample input and ouptut files provided. The dictionary's structure is the following:\n",
    "\n",
    "```python\n",
    "kind_dict = {\"B2\": \"Utility Patent Grant (with a published application) issued on or after January 2, 2001.\", \n",
    "             \"S1\": \"Design Patent\", \n",
    "             \"E1\": \"Reissue Patent\", \n",
    "             \"B1\": \"Utility Patent Grant (no published application) issued on or after January 2, 2001.\",\n",
    "             \"P2\": \"Plant Patent Grant (no published application) issued on or after January 2, 2001\",\n",
    "             \"P3\": \"Plant Patent Grant (with a published application) issued on or after January 2, 2001\"}\n",
    "```\n",
    "The function `getKind` was defined and designed to make use of the dictionary, formatting the output as desired.\n",
    "Regex captures the Key in between the 'kind'-tags and then Key is replaced with the Value from the dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.232348Z",
     "start_time": "2020-07-14T09:56:57.227735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract kind code and translate to desired string using kind dictionary\n",
    "def getKind(data, fileType = 'csv'):\n",
    "    section_pattern = r'<publication-reference.+?>.+?</publication-reference>'\n",
    "    text_section = re.findall(section_pattern, data)\n",
    "    kind_pattern = r'<kind>(.+?)</kind>'\n",
    "    kind_code = re.findall(kind_pattern, ''.join(text_section))\n",
    "    kind_code = ''.join(kind_code[0])\n",
    "    \n",
    "    # Obtaining string-value from dictionary\n",
    "    kind = kind_dict.get(kind_code)\n",
    "    \n",
    "    # Formatting for different output files\n",
    "    if ',' in  kind and fileType != 'json':\n",
    "        kind = '\"' + kind + '\"'\n",
    "    return kind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number_of_claims feature\n",
    "Number of claims data was straightforward. It was found under `<number-of-claims>` tag, and it consisted in a number to be extracted with no further processing.\n",
    "The following `getNumberOfClaims` was designed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.235497Z",
     "start_time": "2020-07-14T09:56:57.233305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract number of claims\n",
    "def getNumberOfClaims(data):\n",
    "    num_of_claims_pattern = r'<number-of-claims>(.+?)</number-of-claims>'\n",
    "    num_of_claims = re.findall(num_of_claims_pattern, data)\n",
    "    num_of_claims = ''.join(num_of_claims[0])\n",
    "    return num_of_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inventors feature\n",
    "This feature data was nested under the `<inventors>` tag, specifically under `<last-name>` and `<first-name>` tags. The regular expression was designed to capture everything inside opening and closing name tags. The function then defined to extract this data performs several actions to get to the desired format.\n",
    "The actions are:\n",
    "- Extraction of the first and last name, compile them into one inventor;\n",
    "- Separating inventors with the comma characters;\n",
    "- Wrapping the list of inventors into square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.241034Z",
     "start_time": "2020-07-14T09:56:57.236385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for extracting inventors. Output is a string enclosed in square brackets\n",
    "def getInventors(data, fileType = 'csv'):\n",
    "    section_pattern = r'<inventors>.+?</inventors>'\n",
    "    text_section = re.findall(section_pattern, data)\n",
    "    inventors_pattern = r'<last-name>(.+?)</last-name><first-name>(.+?)</first-name>'\n",
    "    inventors = re.findall(inventors_pattern, ''.join(text_section))\n",
    "    \n",
    "    # Arranging inventors data based on output sample files\n",
    "    inventors = '[' + ','.join([item[1]+' '+item[0] for item in inventors]) + ']'\n",
    "    \n",
    "    # Formatting for different output files\n",
    "    if ',' in inventors and fileType != 'json':\n",
    "        inventors = '\"' + inventors + '\"'\n",
    "    elif '/' in inventors and fileType == 'json':\n",
    "        inventors = re.sub(r'/', '\\/', inventors)\n",
    "    return inventors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### citations_count features\n",
    "The tag `<us-references-cited>` contained all references for a particular patent. Each citation was defined by a category under `<category>` tag, from where it could be distinguished if it was an examiner or applicant citation. This feature was taken into account and used to define the function `getCitations`, which makes use of the regular expression `cited by applicant` or `cited by examiner` to make the corresponding counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.246497Z",
     "start_time": "2020-07-14T09:56:57.242863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to obtain citations count. \n",
    "# Category argument allows to determine if the count is made for applicants or for examiners.\n",
    "def getCitations(data, category):\n",
    "    section_pattern = r'<us-references-cited>.+?</us-references-cited>'\n",
    "    text_section = re.findall(section_pattern, data)\n",
    "    \n",
    "    # Search pattern for applicant count\n",
    "    if category == 'applicant':\n",
    "        cite_pattern = r'cited by applicant'\n",
    "    \n",
    "    # Search pattern for examiner count\n",
    "    if category == 'examiner':\n",
    "        cite_pattern = r'cited by examiner'\n",
    "    \n",
    "    # Computing number of citations\n",
    "    result = len(re.findall(cite_pattern, ''.join(text_section)))\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claims_text feature\n",
    "This feature was nested inside the tag `<claims>`. The extraction of claims data was an exception to the general procedure, since once obtained the smaller section of text, undesired data was eliminated (cleaning text) instead of extracting relevant data. Undesired data consisted mainly in `<claim-text>` tags and `<claim-ref>` tags inside the claim text. Starting and ending `<claims>` tags were also eliminated, followed by the replacement of `</claim>` tags by a comma as in the output sample files. Finally, last undesired xml tags were eliminated (the method used for finding html tags and special xml characters is detailed in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.252350Z",
     "start_time": "2020-07-14T09:56:57.247929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean claims text.\n",
    "def getClaims(data, fileType = 'csv'):\n",
    "    section_pattern = r'<claims.+?>.+?</claims>'\n",
    "    claims_section = re.findall(section_pattern, data)\n",
    "    \n",
    "    # Cleaning of undesired tags and unwanted text\n",
    "    claim_tags_pattern = r'<claims.+?claim-text>|</?claim-text>|<claim .+?>|<claim-ref.+?>|</claim-ref>|</claims>'\n",
    "    claims_section = re.sub(claim_tags_pattern, '', ''.join(claims_section))\n",
    "    claims_section = [re.sub(r'</claim>', ',', ''.join(claims_section))[:-1]]\n",
    "    \n",
    "    # Cleaning detected xml/html tags\n",
    "    claims_section = [re.sub(r'\\<.+?>', '', ''.join(claims_section))]\n",
    "    \n",
    "    # Formatting according to sample output files\n",
    "    claims_section = \"[\" + ''.join(claims_section) + \"]\"\n",
    "    \n",
    "    # Formatting for different output files\n",
    "    if ',' in claims_section and fileType != 'json':\n",
    "        claims_section = '\"' + claims_section + '\"'\n",
    "    elif '/' in claims_section and fileType == 'json':\n",
    "        claims_section = re.sub(r'/', '\\/', claims_section)\n",
    "    return claims_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abstract feature\n",
    "Finally, the abstract was obtained. It was located under `<abstract>` tag, inside html tags delimited by `<p>` tags. The input file was inspected to assure that every single abstract in it was made up by only one paragraph. It was detected as well that the `<abstract>` tag was sometimes missing. For this cases, a `'NA'` string was imputed. The following `getAbstract` function was defined, which handled undesired html tags found in the text for yielding a clean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.257548Z",
     "start_time": "2020-07-14T09:56:57.253308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for extracting abstract text.\n",
    "def getAbstract(data, fileType = 'csv'):\n",
    "    section_pattern = r'<abstract.+?>.+?</abstract>'\n",
    "    text_section = re.findall(section_pattern, data)\n",
    "    abstract_pattern = r'<p.+?>(.+?)</p>'\n",
    "    abstract = re.findall(abstract_pattern, ''.join(text_section))\n",
    "    \n",
    "    # Imputation of 'NA' string in cases of missing abstract tag\n",
    "    if len(abstract) == 0:\n",
    "        abstract = 'NA'\n",
    "    else:\n",
    "        abstract = ''.join(abstract[0])\n",
    "    \n",
    "    # Formatting for different output files\n",
    "    if ',' in abstract and fileType != 'json':\n",
    "        abstract = '\"' + abstract + '\"'\n",
    "    elif '/' in abstract and fileType == 'json':\n",
    "        abstract = re.sub(r'/', '\\/', abstract)\n",
    "    \n",
    "    # Eliminating undesired html/xml tags\n",
    "    abstract = re.sub(r'<.+?>', '', abstract)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detecting unwanted data in functions output\n",
    "### HTML/XML tags\n",
    "The functions presented in the previous section were designed iteratively, based on interaction with the data and the corresponding function outputs. The first version of the functions didn't take into account handling undesired html/xml tags in the features containing written text (patent_title, claims, abstract, inventors). In order to find out which of these features contained this unwanted data, the first version of the functions where used in the following way to store in lists unwanted tags:\n",
    "```python\n",
    "patent_title = []\n",
    "inventors = []\n",
    "claims = []\n",
    "abstract = []\n",
    "\n",
    "# Search unwanted tags in functions output and append to corresponding lists\n",
    "for item in data:\n",
    "    patent_title.append(''.join(re.findall(r'<.+?>', getPatentTitle(item))))\n",
    "    claims.append(''.join(re.findall(r'<.+?>', getClaims(item))))\n",
    "    abstract.append(''.join(re.findall(r'<.+?>', getAbstract(item))))\n",
    "    inventors.append(''.join(re.findall(r'<.+?>', getInventors(item))))\n",
    "\n",
    "# Data processing to ease data interpretation\n",
    "patent_title = ''.join([c for c in patent_title if c != '']).split('>')\n",
    "claims = ''.join([c for c in claims if c != '']).split('>')\n",
    "abstract = ''.join([c for c in abstract if c != '']).split('>')\n",
    "inventors = ''.join([c for c in inventors if c != '']).split('>')\n",
    "\n",
    "# Print unique unwanted tags\n",
    "print('patent_title:', set(patent_title))\n",
    "print('claims:', set(claims))\n",
    "print('abstract:', set(abstract))\n",
    "print('inventors:', set(inventors))\n",
    "```\n",
    "The following results were obtained, indicating the presence of tag `<i>` in patent title data, and tags `<sub>`, `<i>` and `<b>` in abstract data:\n",
    "```python\n",
    "patent_title: {'', '</i', '<i'}\n",
    "claims: {''}\n",
    "abstract: {'', '<sub', '<i', '</i', '</b', '</sub', '<b'}\n",
    "inventors: {''}\n",
    "```\n",
    "Given these results, the functions where modified to handle these cases and the final version of them were defined as shown in the previous section.\n",
    "\n",
    "### XML special characters\n",
    "A similar approach was applied to detect unwanted xml special characters in the extracted data. In this case the finding was bigger, with 39 different special characters detected. The following was performed:\n",
    "```python\n",
    "patent_title = []\n",
    "inventors = []\n",
    "claims = []\n",
    "abstract = []\n",
    "\n",
    "# Search unwanted special characters in functions output and append to corresponding lists\n",
    "for item in data:\n",
    "    patent_title.append(''.join(re.findall(r'&.+?;', getPatentTitle(item))))\n",
    "    claims.append(''.join(re.findall(r'&.+?;', getClaims(item))))\n",
    "    abstract.append(''.join(re.findall(r'&.+?;', getAbstract(item))))\n",
    "    inventors.append(''.join(re.findall(r'&.+?;', getInventors(item))))\n",
    "\n",
    "# Data processing to ease data interpretation\n",
    "patent_title = ''.join([c for c in patent_title if c != '']).split(';')\n",
    "claims = ''.join([c for c in claims if c != '']).split(';')\n",
    "abstract = ''.join([c for c in abstract if c != '']).split(';')\n",
    "inventors = ''.join([c for c in inventors if c != '']).split(';')\n",
    "\n",
    "# Print unique unwanted xml special characters\n",
    "print('patent_title:', set(patent_title))\n",
    "print('claims:', set(claims))\n",
    "print('abstract:', set(abstract))\n",
    "print('inventors:', set(inventors))\n",
    "```\n",
    "The above yielded the following results:\n",
    "```python\n",
    "patent_title: {'', '&#x2014', '&#x2019', '&#x3b1', '&#x2018'}\n",
    "claims: {'', '&#x2229', '&#x2014', '&#x2205', '&#x2260', '&#x2261', '&#xb7', '&#x3c', '&#x3c4', '&#x2062', '&#x201c', '&#x222a', '&#x3b4', '&#x3b8', '&#x2212', '&#x2003', '&#x2264', '&#x2265', '&#x2207', '&#x2550', '&#x201d', '&#x3c1', '&#x3bd', '&#xb0', '&#x3c0', '&#x2208', '&#xd7', '&#x3b3', '&#x3e', '&#x2202', '&#x394', '&#x2019', '&#x3b1', '&#x2018', '&#x3ba', '&#x212b', '&#x3bc'}\n",
    "abstract: {'', '&#x2014', '&#x2018', '&#x201d', '&#x3c', '&#x2212', '&#x2019', '&#x201c', '&#x2264', '&#x3b1', '&#xb0', '&#x3bc', '&#x3b4', '&#x3b3', '&#x3e'}\n",
    "inventors: {'', '&#xe9', '&#xf4', '&#xe7'}\n",
    "```\n",
    "A dictionary was generated to handle these special characters based on the information contained in http://www.howtocreate.co.uk/sidehtmlentity.html. This page contained all the symbols corresponding to every special character detected. The dictionary was the following, used later in the extracting/writing output CSV (not JSON) file:\n",
    "```python\n",
    "# xml special characters dictionary\n",
    "special_char_dict = {'&#x2003;':' ', '&#x2014;':'—', '&#x2018;':'‘', '&#x2019;':'’', '&#x201c;':'“',\n",
    "'&#x201d;':'”','&#x2062;':'', '&#x212b;':'Å', '&#x2202;':'∂', '&#x2205;':'∅', '&#x2207;':'∇', '&#x2208;':'∈','&#x2212;':'−', '&#x2229;':'∩', '&#x222a;':'∪', '&#x2260;':'≠', '&#x2261;':'≡', '&#x2264;':'≤', '&#x2265;':'≥', '&#x2550;':'═', '&#x394;':'Δ', '&#x3b1;':'α', '&#x3b3;':'γ', '&#x3b4;':'δ', '&#x3b8;':'θ', '&#x3ba;':'κ', '&#x3bc;':'μ', '&#x3bd;':'ν', '&#x3c;':'<', '&#x3c0;':'π', '&#x3c1;':'ρ', '&#x3c4;':'τ', '&#x3e;':'>', '&#xb0;':'°', '&#xb7;':'·', '&#xd7;':'×', '&#xe7;':'ç', '&#xe9;':'é', '&#xf4;':'ô'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting data / writing output files\n",
    "Finally, all dictionaries and functions presented above were combined in corresponding `for loops` to extract and clean the relevant data and immediately write the output to csv and json files. The procedure is described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.260837Z",
     "start_time": "2020-07-14T09:56:57.258613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating kind dictionary needed for translating kind codes to desired text\n",
    "kind_dict = {\"B2\": \"Utility Patent Grant (with a published application) issued on or after January 2, 2001.\", \n",
    "               \"S1\": \"Design Patent\", \"E1\": \"Reissue Patent\", \n",
    "               \"B1\": \"Utility Patent Grant (no published application) issued on or after January 2, 2001.\",\n",
    "              \"P2\": \"Plant Patent Grant (no published application) issued on or after January 2, 2001\",\n",
    "              \"P3\": \"Plant Patent Grant (with a published application) issued on or after January 2, 2001\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.266249Z",
     "start_time": "2020-07-14T09:56:57.261808Z"
    }
   },
   "outputs": [],
   "source": [
    "# xml special character dictionary\n",
    "special_char_dict = {'&#x2003;':' ', '&#x2014;':'—', '&#x2018;':'‘', '&#x2019;':'’', '&#x201c;':'“', '&#x201d;':'”', '&#x2062;':'', \\\n",
    "                     '&#x212b;':'Å', '&#x2202;':'∂', '&#x2205;':'∅', '&#x2207;':'∇', '&#x2208;':'∈', '&#x2212;':'−', '&#x2229;':'∩',\\\n",
    "                      '&#x222a;':'∪', '&#x2260;':'≠', '&#x2261;':'≡', '&#x2264;':'≤', '&#x2265;':'≥', '&#x2550;':'═', '&#x394;':'Δ',\\\n",
    "                      '&#x3b1;':'α', '&#x3b3;':'γ', '&#x3b4;':'δ', '&#x3b8;':'θ', '&#x3ba;':'κ', '&#x3bc;':'μ', '&#x3bd;':'ν',\\\n",
    "                     '&#x3c;':'<', '&#x3c0;':'π', '&#x3c1;':'ρ', '&#x3c4;':'τ', '&#x3e;':'>', '&#xb0;':'°', '&#xb7;':'·', '&#xd7;':'×',\\\n",
    "                      '&#xe7;':'ç', '&#xe9;':'é', '&#xf4;':'ô'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.276677Z",
     "start_time": "2020-07-14T09:56:57.267464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Opening output file handlers\n",
    "output_csv = open('Group135.csv', mode = 'w', encoding='utf-8')\n",
    "output_json = open('Group135.json', mode = 'w', encoding='utf-8')\n",
    "\n",
    "# Adding CSV file header and JSON opening curly bracket\n",
    "output_csv.write(\"grant_id,patent_title,kind,number_of_claims,inventors,citations_applicant_count,citations_examiner_count,claims_text,abstract\\n\")\n",
    "output_json.write(\"{\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T09:56:57.923188Z",
     "start_time": "2020-07-14T09:56:57.277906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Writing JSON file while extracting data\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    # Calling functions to get data for JSON output file\n",
    "    grant_id = getGrantId(data[i])\n",
    "    patent_title = getPatentTitle(data[i], fileType = 'json')\n",
    "    kind = getKind(data[i], fileType = 'json')\n",
    "    number_of_claims = getNumberOfClaims(data[i])\n",
    "    inventors = getInventors(data[i], fileType = 'json')\n",
    "    citations_applicant = getCitations(data[i], category = 'applicant')\n",
    "    citations_examiner = getCitations(data[i], category = 'examiner')\n",
    "    claims = getClaims(data[i], fileType = 'json')\n",
    "    abstract = getAbstract(data[i], fileType = 'json')\n",
    "\n",
    "    \n",
    "    # Writing output data to file\n",
    "    output_json.write('\"' + grant_id + '\"' + \":{\" \\\n",
    "                      + '\"patent_title\"' + \":\" + '\"' + patent_title + '\"' + \",\" \\\n",
    "                      + '\"kind\"' + \":\" + '\"' + kind + '\"' + \",\" \\\n",
    "                      + '\"number_of_claims\"' + \":\" + number_of_claims + \",\" \\\n",
    "                      + '\"inventors\"' + \":\" + '\"' + inventors + '\"' + \",\" \\\n",
    "                      + '\"citations_applicant_count\"' + \":\" + citations_applicant + \",\" \\\n",
    "                      + '\"citations_examiner_count\"' + \":\" + citations_examiner + \",\" \\\n",
    "                      + '\"claims_text\"' + \":\" + '\"' + claims + '\"' + \",\" \\\n",
    "                      + '\"abstract\"' + \":\" + '\"' + abstract + '\"' \\\n",
    "                      + \"}\" \\\n",
    "                     )\n",
    "\n",
    "\n",
    "    if i == len(data)-1:\n",
    "        output_json.write(\"}\") #closing curly bracket\n",
    "    else:\n",
    "        output_json.write(\",\") #comma separating patent grants\n",
    "        \n",
    "# Writing CSV file while extracting data\n",
    "data_csv = data\n",
    "\n",
    "# Replacing all special characters for csv file\n",
    "for i in range(len(data_csv)):\n",
    "    for (key, value) in special_char_dict.items():\n",
    "        if key in data_csv[i]:\n",
    "            data_csv[i] = data_csv[i].replace(key, value)\n",
    "\n",
    "# Writing CSV file while extracting data\n",
    "for i in range(len(data_csv)):\n",
    "            \n",
    "    # Calling functions to get data for CSV output file\n",
    "    grant_id = getGrantId(data_csv[i])\n",
    "    patent_title = getPatentTitle(data_csv[i])\n",
    "    kind = getKind(data_csv[i])\n",
    "    number_of_claims = getNumberOfClaims(data_csv[i])\n",
    "    inventors = getInventors(data_csv[i])\n",
    "    citations_applicant = getCitations(data_csv[i], category = 'applicant')\n",
    "    citations_examiner = getCitations(data_csv[i], category = 'examiner')\n",
    "    claims = getClaims(data_csv[i])\n",
    "    abstract = getAbstract(data_csv[i])\n",
    "    \n",
    "    # CSV output lines\n",
    "    output_csv.write(grant_id + \",\" \\\n",
    "                     + patent_title + \",\" \\\n",
    "                     + kind + \",\" \\\n",
    "                     + number_of_claims + \",\" \\\n",
    "                     + inventors + \",\" \\\n",
    "                     + citations_applicant + \",\" \\\n",
    "                     + citations_examiner + \",\" \\\n",
    "                     + claims + \",\" \\\n",
    "                     + abstract \\\n",
    "                     )\n",
    "    if i != len(data_csv)-1:\n",
    "        output_csv.write(\"\\n\") #line break for csv lines except last one\n",
    "\n",
    "\n",
    "#Closing file handlers\n",
    "output_csv.close()\n",
    "output_json.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "This assessment measured the understanding of XML, CSV and JSON file structure and ability to extract the information from the XML format and write to CSV and JSON formats. This assignment could be conditionally divided into the following blocks:\n",
    "\n",
    "- **XML parsing and data extraction**. After exploring the input file and given samples, the general boundaries (HTML-tags) were identified for the following data extraction. The needed content was accessed using `re` library for regular expressions. \n",
    "- **Dealing with HTML/XML tags**. Some HTML/XML special characters were detected in the textual content. We used an external HTML entity reference in purpose to generate the dictionary for replacing them with the special symbols for CSV file. \n",
    "- **Output file structure compliance**. Output files have a specific framework. For example, in JSON file slash `/` should be represented as `\\/`, which is critical for its pattern. Or excessive comma characters in a textual value of a particular feature can violate the structure of the CSV file. To avoid this, the textual value for features `patent_title, kind, inventors, claims_text, abstract` should be wrapped into quota marks. Some manipulations with the extracted values were executed on a data processing stage.\n",
    "- **Exporting data to a specific format**. Two output file in CSV and JSON format were created using python file handlers. Before writing the content both files should be prepared: CSV file needs a header, JSON file starts with \"{\" sign. Then two iterative functions executed extracting of needed content and writing it into the files. Before closing the files we should avoid the creation of a new line in a CSV file and should close the bracket \"}\" for JSON. The two output files are closed and ready for further reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "- Python Software Foundation, 2019. *re — Regular expression operations*. Retrieved from https://docs.python.org/3/library/re.html\n",
    "- IBM Knowledge Center. *Markdown for Jupyter notebooks cheatsheet*. Retrieved from https://www.ibm.com/support/knowledgecenter/en/SSGNPV_1.1.3/dsx/markd-jupyter.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
